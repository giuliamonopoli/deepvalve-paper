{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,re\n",
    "\n",
    "import config\n",
    "import cv2\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import utils as ut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" Count number of Patient available\"\n",
    "elements = os.listdir(config.annotation_folder)\n",
    "len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_imgs_path=config.raw_imgs_path,\n",
    "box_data_path=config.box_data_path,\n",
    "annotation_folder=config.annotation_folder\n",
    "\n",
    "def get_bounding_box_for_patient(data, target_patient_name):\n",
    "    for entry in data:\n",
    "        if entry[\"patient_name\"] == target_patient_name:\n",
    "            bounding_box = entry.get(\"bounding_box\", None)\n",
    "            return bounding_box\n",
    "    return None\n",
    "\n",
    "# # Load data from JSON file\n",
    "# json_file_path = \"/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/annotations_old.json\"  # Replace with the actual path to your JSON file\n",
    "# with open(json_file_path, \"r\") as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "# # Example usage:\n",
    "# target_patient_name = \"MAD_102_0\"  # Replace with the actual patient name\n",
    "# bounding_box = get_bounding_box_for_patient(data, target_patient_name)\n",
    "\n",
    "# if bounding_box is not None:\n",
    "#     print(f\"Bounding box for {target_patient_name}: {bounding_box}\")\n",
    "# else:\n",
    "#     print(f\"No data found for {target_patient_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation( file_path):\n",
    "    \"\"\"\n",
    "    Parse the annotation file and extract relevant information.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the annotation file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed annotation data.\n",
    "    \"\"\"\n",
    "    with open(file_path) as file:\n",
    "        lines = file.read().split(\"\\n\")\n",
    "\n",
    "    # Initialize variables and dictionaries\n",
    "    annotation_data = {}\n",
    "    key_frames = set()\n",
    "    patient_name = lines[1]\n",
    "    patient_name = (\n",
    "        patient_name.split(\"/\")[-3]\n",
    "        + \"_\"\n",
    "        + patient_name.split(\"/\")[-2].split(\"_\")[-1]\n",
    "    )  # to get patientname_0/1/2 if multiple images present for the same patient\n",
    "    error_list = [\"no_error\"]\n",
    "\n",
    "    # Extract error information\n",
    "    for line in lines:\n",
    "        error_line = lines[2]\n",
    "        if error_line.strip() == \"\":\n",
    "            error_list = [\"no_error\"]\n",
    "        # Check for lines with an asterisk (*) indicating errors\n",
    "        else:\n",
    "            error_list = error_line.split(\"<br>\") if \"<br>\" in error_line else [error_line]\n",
    "            # Remove leading and trailing whitespace from each error\n",
    "            error_list = [error.strip() for error in error_list if error.strip() != \"\"]\n",
    "       \n",
    "        if \"mv_insert\" in line:\n",
    "            key_frame = line.split(\" \")[0]\n",
    "            key_frames.add(key_frame)\n",
    "\n",
    "\n",
    "    annotation_data[\"patient_name\"] = patient_name\n",
    "    annotation_data[\"flags\"] = error_list\n",
    "    annotation_data[\"key_frames\"] = list(key_frames)\n",
    "    annotation_data[\"annotations\"] = {}\n",
    "    json_file_path = \"/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/annotations_old.json\"  \n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    annotation_data[\"bounding_box\"] = list(get_bounding_box_for_patient(data, patient_name))\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip().split()\n",
    "\n",
    "        # Check if the line starts with a key frame\n",
    "        if line and line[0] in key_frames:\n",
    "            frame = line[0]\n",
    "            key = line[1]\n",
    "            values = list(tuple(map(int, line[2:])))\n",
    "\n",
    "            # Initialize nested dictionaries if necessary\n",
    "            if frame not in annotation_data[\"annotations\"]:\n",
    "                annotation_data[\"annotations\"][frame] = {}\n",
    "            if key not in annotation_data[\"annotations\"][frame]:\n",
    "                annotation_data[\"annotations\"][frame][key] = []\n",
    "\n",
    "            # Append values to the appropriate key frame and key\n",
    "            annotation_data[\"annotations\"][frame][key].append(values)\n",
    "\n",
    "    # Return the populated annotation data dictionary\n",
    "    return annotation_data\n",
    "\n",
    "\n",
    "\n",
    "# parse all the txt files for all patients in a json and write them to a file\n",
    "def get_annotation_json(folder_path=\"export\"):\n",
    "    \"\"\"\n",
    "    Parse all the text files in the specified folder and return the annotation data.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing annotation files.\n",
    "\n",
    "    Returns:\n",
    "        list: List of annotation data dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    annotation_data_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # print(files)\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    annotation_data = parse_annotation(file_path)\n",
    "                    print(annotation_data)\n",
    "                    \n",
    "                    annotation_data_list.append(annotation_data)\n",
    "                except Exception as e:\n",
    "                    Exception(\"Error parsing annotation file: \", file_path, e)\n",
    "    \n",
    "    with open(config.annotation_json, \"w\") as json_file:\n",
    "        json.dump(annotation_data_list, json_file, indent=4)\n",
    "\n",
    "\n",
    "annotation_folder_path = \"/Users/giuliamonopoli/Desktop/PhD /deepvalve/AW_MAD-redo_NF_20231121\"\n",
    "\n",
    "get_annotation_json(annotation_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kframes_and_annot_from_mhd( list_p,annotation_data_list, dataset=\"MAD\"):\n",
    "        \"\"\"\n",
    "        Retrieves key frames and corresponding annotations from MHD files.\n",
    "\n",
    "        Args:\n",
    "            annotation_data_list (list): List of annotation data dictionaries.\n",
    "            dataset (str, optional): Dataset identifier. Defaults to \"MAD\".\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing two lists - `lst_of_matrix_imgs` and `lst_of_annotation_imgs`.\n",
    "                `lst_of_matrix_imgs` (list): List of matrix images.\n",
    "                `lst_of_annotation_imgs` (list): List of corresponding annotations.\n",
    "        \"\"\"\n",
    "        lst_of_matrix_imgs = []\n",
    "        lst_of_annotation_imgs = []\n",
    "        lst_of_error_codes = []\n",
    "        lst_of_patient_names = []\n",
    "        raw_imgs_path = config.raw_imgs_path\n",
    "\n",
    "      \n",
    "        for patient in list_p:\n",
    "            print(patient)\n",
    "        \n",
    "                \n",
    "            patient_annotation_data = [\n",
    "                annotation_data\n",
    "                for annotation_data in annotation_data_list\n",
    "                if annotation_data[\"patient_name\"] == patient \n",
    "            ][\n",
    "                0\n",
    "            ] \n",
    "            # print(patient_annotation_data)\n",
    "            key_frames = patient_annotation_data[\"key_frames\"]\n",
    "            print(key_frames)\n",
    "            center_x, center_y, width, height = patient_annotation_data[\n",
    "                \"bounding_box\"\n",
    "            ]\n",
    "\n",
    "            max_width = max([x[\"bounding_box\"][2] for x in annotation_data_list])\n",
    "            max_height = max([x[\"bounding_box\"][3] for x in annotation_data_list])\n",
    "\n",
    "            for structure_folder in os.listdir(\n",
    "                os.path.join(raw_imgs_path, patient[:-2])\n",
    "            ):\n",
    "                if structure_folder.startswith(\"LA\"):  \n",
    "                    \n",
    "                    for file in os.listdir(\n",
    "                        os.path.join(\n",
    "                            raw_imgs_path, patient[:-2], structure_folder\n",
    "                        )\n",
    "                    ):\n",
    "                        file_frame = file.split(\"_\")[1].split(\".\")[0]\n",
    "                        \n",
    "                        if file.endswith(\".mhd\") and file_frame in key_frames:\n",
    "                            print(file,file_frame)\n",
    "                        \n",
    "                            itkimage = sitk.ReadImage(\n",
    "                                os.path.join(\n",
    "                                    raw_imgs_path,\n",
    "                                    patient[:-2],\n",
    "                                    structure_folder,\n",
    "                                    file,\n",
    "                                )\n",
    "                            )\n",
    "                            array_img = sitk.GetArrayFromImage(itkimage)\n",
    "\n",
    "                            # crop array\n",
    "                            x_min = center_x - width // 2\n",
    "                            x_max = center_x + width // 2\n",
    "                            y_min = center_y - height // 2\n",
    "                            y_max = center_y + height // 2\n",
    "\n",
    "                            array_img = array_img[(y_min):(y_max), (x_min):(x_max)]\n",
    "\n",
    "                            # # rescale image\n",
    "                            array_img = cv2.resize(\n",
    "                                array_img,\n",
    "                                (max_width, max_height),\n",
    "                                interpolation=cv2.INTER_CUBIC,\n",
    "                            )\n",
    "\n",
    "                            lst_of_matrix_imgs.append(array_img)\n",
    "                            lst_of_annotation_imgs.append(\n",
    "                                patient_annotation_data[\"annotations\"][file_frame]\n",
    "                            )\n",
    "                            \n",
    "                            \n",
    "                            lst_of_patient_names.append(\n",
    "                                patient_annotation_data[\"patient_name\"]\n",
    "                            )\n",
    "                            \n",
    "                            \n",
    "\n",
    "        # assert (\n",
    "        #     len(lst_of_matrix_imgs)\n",
    "        #     == len(lst_of_annotation_imgs)\n",
    "            \n",
    "        #     == len(lst_of_patient_names)\n",
    "        # ), \"Data length mismatch\"\n",
    "        # return (\n",
    "        #     lst_of_matrix_imgs,\n",
    "        #     lst_of_annotation_imgs,\n",
    "            \n",
    "        #     lst_of_patient_names,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD_178_0\n",
      "['12', '6']\n",
      "frame_6.mhd 6\n",
      "frame_12.mhd 12\n",
      "MAD_149_0\n",
      "['5']\n",
      "frame_5.mhd 5\n",
      "MAD_31_0\n",
      "['13', '7']\n",
      "frame_13.mhd 13\n",
      "frame_7.mhd 7\n",
      "MAD_176_0\n",
      "['12', '19']\n",
      "frame_19.mhd 19\n",
      "frame_12.mhd 12\n",
      "MAD_182_0\n",
      "['4']\n",
      "frame_4.mhd 4\n",
      "MAD_171_0\n",
      "['5']\n",
      "frame_5.mhd 5\n",
      "MAD_62_0\n",
      "['0', '7']\n",
      "frame_0.mhd 0\n",
      "frame_7.mhd 7\n",
      "MAD_96_0\n",
      "['17', '7']\n",
      "frame_17.mhd 17\n",
      "frame_7.mhd 7\n",
      "MAD_91_0\n",
      "['6']\n",
      "frame_6.mhd 6\n"
     ]
    }
   ],
   "source": [
    "with open(config.annotation_json, \"r\") as json_file:\n",
    "            annotation_data_list = json.load(json_file)\n",
    "patients = ['MAD_178_0', 'MAD_149_0', 'MAD_31_0', 'MAD_176_0', 'MAD_182_0', 'MAD_171_0', 'MAD_62_0', 'MAD_96_0', 'MAD_91_0']\n",
    "get_kframes_and_annot_from_mhd(patients,annotation_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Names:\n",
      "['MAD_178_0', 'MAD_149_0', 'MAD_31_0', 'MAD_176_0', 'MAD_182_0', 'MAD_171_0', 'MAD_62_0', 'MAD_96_0', 'MAD_91_0', 'MAD_65_0', 'MAD_53_0', 'MAD_54_0', 'MAD_39_0', 'MAD_4_0', 'MAD_146_0', 'MAD_3_0', 'MAD_112_0', 'MAD_123_0', 'MAD_63_0', 'MAD_139_0', 'MAD_106_0', 'MAD_108_0', 'MAD_84_0', 'MAD_137_0', 'MAD_130_0', 'MAD_12_0', 'MAD_152_0', 'MAD_109_0', 'MAD_136_0', 'MAD_78_0', 'MAD_107_0', 'MAD_162_0', 'MAD_121_0', 'MAD_92_0', 'MAD_66_0', 'MAD_126_0', 'MAD_95_0', 'MAD_50_0', 'MAD_188_0', 'MAD_1_0', 'MAD_144_0', 'MAD_35_0', 'MAD_175_0', 'MAD_181_0', 'MAD_129_0', 'MAD_127_0', 'MAD_118_0', 'MAD_58_0', 'MAD_120_0', 'MAD_180_0', 'MAD_145_0', 'MAD_142_0', 'MAD_160_0', 'MAD_27_0', 'MAD_167_0', 'MAD_158_0', 'MAD_151_0', 'MAD_169_0', 'MAD_102_0', 'MAD_105_0', 'MAD_73_0', 'MAD_17_0', 'MAD_21_0', 'MAD_161_0', 'MAD_19_0', 'MAD_75_0', 'MAD_132_0', 'MAD_103_0']\n",
      "\n",
      "Flags List:\n",
      "[[1, 0, 1, 0, 0, 1], [0, 1, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0], [0, 1, 0, 1, 0, 0], [1, 0, 0, 1, 0, 0], [0, 0, 1, 1, 0, 1], [0, 1, 1, 1, 0, 1], [0, 1, 0, 1, 0, 1], [0, 0, 1, 0, 1, 1], [1, 0, 0, 1, 0, 0], [1, 0, 1, 0, 1, 1], [1, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [1, 1, 1, 0, 0, 1], [0, 0, 1, 0, 1, 1], [0, 0, 1, 1, 0, 1], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 1, 1, 1, 0, 1], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 1], [0, 0, 1, 0, 1, 1], [0, 0, 0, 1, 0, 1], [1, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 1, 1], [0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 0], [1, 1, 0, 0, 0, 1], [0, 0, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1], [1, 0, 0, 0, 1, 0], [0, 1, 0, 1, 0, 0], [1, 0, 1, 1, 1, 1], [1, 0, 0, 1, 0, 1], [0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 1], [1, 1, 0, 1, 0, 1], [0, 0, 1, 1, 0, 1], [1, 0, 1, 0, 0, 1], [0, 0, 0, 1, 0, 0], [0, 0, 1, 1, 0, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 1, 0], [1, 0, 1, 1, 1, 1], [0, 0, 1, 1, 0, 1], [0, 1, 1, 0, 0, 1], [0, 0, 0, 1, 0, 0], [0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 0, 1], [1, 0, 1, 1, 0, 1], [1, 0, 0, 1, 0, 0], [0, 0, 1, 1, 0, 1], [0, 1, 1, 1, 0, 1], [0, 0, 1, 1, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv')  \n",
    "\n",
    "# Extract patient names and flags for each patient\n",
    "patient_names = df['patient_name'].tolist()\n",
    "flags_list = df[['fp_1', 'fp_2', 'fp_3', 'ff_1', 'ff_2', 'ff_3']].values.tolist()\n",
    "\n",
    "# Print the results\n",
    "print(\"Patient Names:\")\n",
    "print(patient_names)\n",
    "\n",
    "print(\"\\nFlags List:\")\n",
    "print(flags_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def split_data():\n",
    "\n",
    "    \" Split data with a new multiclass stratification method \" \n",
    "\n",
    "    df = pd.read_csv('/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv')  \n",
    "\n",
    "    # Extract patient names and flags for each patient\n",
    "    patient_names = df['patient_name'].tolist()\n",
    "    flags_list = df[['fp_1', 'fp_2', 'fp_3', 'ff_1', 'ff_2', 'ff_3']].values.tolist()\n",
    "    \n",
    "    # print(flags_list)\n",
    "    patient_names = np.array(patient_names)\n",
    "    patient_names = patient_names.reshape(-1, 1)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    flags_list = np.array(flags_list)\n",
    "    patient_names_train, flags_train, patient_names_test,flags_test = iterative_train_test_split(patient_names, flags_list, test_size = 0.2)\n",
    "    patient_names_train, flags_train, patient_names_val, flags_val = iterative_train_test_split(patient_names_train, flags_train, test_size = 0.2)\n",
    "\n",
    "    # return patient_names_train, patient_names_test,flags_train,flags_test\n",
    "    \n",
    "    return patient_names_train,patient_names_val, patient_names_test,flags_train,flags_val,flags_test\n",
    "\n",
    "\n",
    "patient_names_train,patient_names_val, patient_names_test,flags_train,flags_val,flags_test = split_data()\n",
    "\n",
    "# patient_names_train,patient_names_val, patient_names_test,flags_train,flags_val,flags_test = split_data()\n",
    "# len(patient_names_test),len(patient_names_train), 14/54\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been splitted into 0.6323529411764706 for training, 0.16176470588235295 for validation, and 0.19117647058823528 for test.\n",
      "The data has been splitted into 43 for training, 11 for validation, and 13 for test.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "\" Method 1\"\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['patient_name'].tolist(), df[['fp_1', 'fp_3', 'ff_1', 'ff_2', 'ff_3']].values.tolist()\n",
    "\n",
    "def get_combinations_counts(flags_list):\n",
    "    class_counts = Counter(map(tuple, flags_list))\n",
    "    filtered_classes = [list(key) for key, value in class_counts.items() if value > 1]\n",
    "    one_class = [list(key) for key, value in class_counts.items() if value == 1]\n",
    "    return filtered_classes, one_class\n",
    "\n",
    "def split_data(patient_names, flags_list,filtered_classes, one_class, test_size=0.3,step=\"one\", random_state=47):\n",
    "    flags_list = np.array(flags_list)\n",
    "    patient_names = np.array(patient_names)\n",
    "\n",
    "    indices = [index for value in filtered_classes for index, name in enumerate(flags_list) if np.array_equal(name, np.array(value))]\n",
    "    indices_one = [index for value in one_class for index, name in enumerate(flags_list) if np.array_equal(name, np.array(value))]\n",
    "\n",
    "    selected_flags = flags_list[indices]\n",
    "    selected_patients = patient_names[indices]\n",
    "\n",
    "    patients_one = patient_names[indices_one]\n",
    "    flags_one = flags_list[indices_one]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_patients, selected_flags, test_size=test_size, random_state=random_state, stratify=selected_flags)\n",
    "    \n",
    "    if step == 'one':\n",
    "        X_test = np.append(X_test, patients_one)\n",
    "        y_test = np.append(y_test, flags_one, axis=0)\n",
    "    elif step == \"second\":\n",
    "        np.random.shuffle(patients_one)\n",
    "        split_index = len(patients_one) // 2\n",
    "        X_train = np.append(X_train,patients_one[:split_index])\n",
    "        y_train = np.append(y_train, flags_one[:split_index], axis=0)\n",
    "        X_test = np.append(X_test,patients_one[:split_index])\n",
    "        y_test = np.append(y_test, flags_one[split_index:], axis=0)\n",
    "        \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def process_and_split_data(file_path, test_size=0.3, random_state=47):\n",
    "    patient_names, flags_list = load_data(file_path)\n",
    "    filtered_classes, one_class = get_combinations_counts(flags_list)\n",
    "    return split_data(patient_names, flags_list,filtered_classes, one_class, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = process_and_split_data('/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv')\n",
    "# filtered_classes_t, one_class_t = get_combinations_counts(y_train)\n",
    "filtered_classes_t, one_class_t = get_combinations_counts(y_test)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = split_data(X_train, y_train,filtered_classes_t, one_class_t, test_size=0.3,step=\"second\", random_state=47)\n",
    "X_test, X_val, y_test, y_val = split_data(X_test, y_test,filtered_classes_t, one_class_t, test_size=0.3,step=\"second\", random_state=47)\n",
    "\n",
    "print(f\"The data has been splitted into {len(X_train)/68} for training, {len(X_val)/68} for validation, and {len(X_test)/68} for test.\")\n",
    "\n",
    "\n",
    "print(f\"The data has been splitted into {len(X_train)} for training, {len(X_val)} for validation, and {len(X_test)} for test.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     filtered_classes, one_class \u001b[39m=\u001b[39m get_combinations_counts(flags_list)\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m split_data(patient_names, flags_list,filtered_classes, one_class, test_size\u001b[39m=\u001b[39mtest_size, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m---> 53\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m process_and_split_data(\u001b[39m'\u001b[39;49m\u001b[39m/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     54\u001b[0m \u001b[39m# filtered_classes_t, one_class_t = get_combinations_counts(y_train)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m filtered_classes_t, one_class_t \u001b[39m=\u001b[39m get_combinations_counts(y_test)\n",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m, in \u001b[0;36mprocess_and_split_data\u001b[0;34m(file_path, test_size, random_state)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_and_split_data\u001b[39m(file_path, test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m47\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     patient_names, flags_list \u001b[39m=\u001b[39m load_data(file_path)\n\u001b[1;32m     49\u001b[0m     filtered_classes, one_class \u001b[39m=\u001b[39m get_combinations_counts(flags_list)\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m split_data(patient_names, flags_list,filtered_classes, one_class, test_size\u001b[39m=\u001b[39mtest_size, random_state\u001b[39m=\u001b[39mrandom_state)\n",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(file_path):\n\u001b[0;32m----> 9\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path)\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mpatient_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(), df[[\u001b[39m'\u001b[39m\u001b[39mfp_1\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m'\u001b[39m\u001b[39mfp_3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mff_1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mff_2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mff_3\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\" Modification of first method\"\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "\" Method 1\"\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['patient_name'].tolist(), df[['fp_1',  'fp_3', 'ff_1', 'ff_2', 'ff_3']].values.tolist()\n",
    "\n",
    "def get_combinations_counts(flags_list):\n",
    "    class_counts = Counter(map(tuple, flags_list))\n",
    "    filtered_classes = [list(key) for key, value in class_counts.items() if value > 1]\n",
    "    one_class = [list(key) for key, value in class_counts.items() if value == 1]\n",
    "    return filtered_classes, one_class\n",
    "\n",
    "def split_data(patient_names, flags_list,filtered_classes, one_class, test_size=0.3,step=\"one\", random_state=47):\n",
    "    flags_list = np.array(flags_list)\n",
    "    patient_names = np.array(patient_names)\n",
    "\n",
    "    indices = [index for value in filtered_classes for index, name in enumerate(flags_list) if np.array_equal(name, np.array(value))]\n",
    "    indices_one = [index for value in one_class for index, name in enumerate(flags_list) if np.array_equal(name, np.array(value))]\n",
    "\n",
    "    selected_flags = flags_list[indices]\n",
    "    selected_patients = patient_names[indices]\n",
    "\n",
    "    patients_one = patient_names[indices_one]\n",
    "    flags_one = flags_list[indices_one]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_patients, selected_flags, test_size=test_size, random_state=random_state, stratify=selected_flags)\n",
    "    \n",
    "    if step == 'one':\n",
    "        X_test = np.append(X_test, patients_one)\n",
    "        y_test = np.append(y_test, flags_one, axis=0)\n",
    "    elif step == \"second\":\n",
    "        np.random.shuffle(patients_one)\n",
    "        split_index = len(patients_one) // 2\n",
    "        X_train = np.append(X_train,patients_one[:split_index])\n",
    "        y_train = np.append(y_train, flags_one[:split_index], axis=0)\n",
    "        X_test = np.append(X_test,patients_one[:split_index])\n",
    "        y_test = np.append(y_test, flags_one[split_index:], axis=0)\n",
    "        \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def process_and_split_data(file_path, test_size=0.25, random_state=47):\n",
    "    patient_names, flags_list = load_data(file_path)\n",
    "    filtered_classes, one_class = get_combinations_counts(flags_list)\n",
    "    return split_data(patient_names, flags_list,filtered_classes, one_class, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = process_and_split_data('/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv')\n",
    "# filtered_classes_t, one_class_t = get_combinations_counts(y_train)\n",
    "filtered_classes_t, one_class_t = get_combinations_counts(y_test)\n",
    "k_folds = 5 \n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "print(len(X_train))\n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_train)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "   \n",
    "    print(train_subsampler)\n",
    "# print(f\"The data has been split into {len(X_train)/68} for training, {len(X_val)/68} for validation, and {len(X_test)/68} for test.\")\n",
    "\n",
    "# print(f\"The data has been splitted into {len(X_train)/68} for training, {len(X_val)/68} for validation, and {len(X_test)/68} for test.\")\n",
    "\n",
    "\n",
    "# print(f\"The data has been splitted into {len(X_train)} for training, {len(X_val)} for validation, and {len(X_test)} for test.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m split_data(patient_names, flags_list,filtered_classes, one_class, test_size\u001b[39m=\u001b[39mtest_size, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m     41\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m process_and_split_data(\u001b[39m'\u001b[39m\u001b[39m/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,  random_state\u001b[39m=\u001b[39m\u001b[39m47\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold\n\u001b[1;32m     46\u001b[0m scores\u001b[39m=\u001b[39m[]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "\" Method 2\"\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['patient_name'].tolist(), df[['fp_1', 'fp_2', 'fp_3', 'ff_1', 'ff_2', 'ff_3']].values.tolist()\n",
    "\n",
    "def get_combinations_counts(flags_list):\n",
    "    class_counts = Counter(map(tuple, flags_list))\n",
    "    filtered_classes = [list(key) for key, value in class_counts.items() if value > 1]\n",
    "    one_class = [list(key) for key, value in class_counts.items() if value == 1]\n",
    "    return filtered_classes, one_class\n",
    "\n",
    "def split_data(patient_names, flags_list,filtered_classes, one_class, test_size=0.3, random_state=47):\n",
    "    flags_list = np.array(flags_list)\n",
    "    patient_names = np.array(patient_names)\n",
    "\n",
    "    indices = [index for value in filtered_classes for index, name in enumerate(flags_list) if np.array_equal(name, np.array(value))]\n",
    "    indices_one = [index for value in one_class for index, name in enumerate(flags_list) if np.array_equal(name, np.array(value))]\n",
    "\n",
    "    selected_flags = flags_list[indices]\n",
    "    selected_patients = patient_names[indices]\n",
    "\n",
    "    patients_one = patient_names[indices_one]\n",
    "    flags_one = flags_list[indices_one]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_patients, selected_flags, test_size=test_size, random_state=random_state, stratify=selected_flags)\n",
    "\n",
    "    X_train = np.append(X_train, patients_one)\n",
    "    y_train = np.append(y_train, flags_one, axis=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def process_and_split_data(file_path, test_size=0.3, random_state=47):\n",
    "    patient_names, flags_list = load_data(file_path)\n",
    "    filtered_classes, one_class = get_combinations_counts(flags_list)\n",
    "    return split_data(patient_names, flags_list,filtered_classes, one_class, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = process_and_split_data('/Users/giuliamonopoli/Desktop/PhD /deepvalve/data/data_new.csv')\n",
    "\n",
    "X_train, X_val, y_train, y_val = KFold(n_splits= 2,  random_state=47)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set distribution: Counter({(0, 0, 1, 1, 0, 1): 4, (0, 0, 1, 0, 1, 1): 3, (0, 1, 0, 1, 0, 0): 3, (0, 0, 0, 0, 0, 0): 3, (1, 0, 0, 0, 0, 0): 2, (0, 0, 1, 1, 1, 1): 2, (0, 0, 0, 1, 0, 0): 2, (0, 1, 1, 1, 0, 1): 1, (0, 0, 1, 0, 0, 1): 1, (1, 0, 0, 1, 0, 0): 1, (1, 0, 1, 0, 0, 1): 1, (1, 0, 1, 1, 1, 1): 1, (0, 0, 0, 1, 1, 0): 1, (1, 0, 0, 1, 1, 1): 1, (0, 0, 0, 0, 0, 1): 1, (0, 1, 0, 1, 1, 0): 1, (0, 1, 0, 1, 0, 1): 1, (1, 0, 1, 0, 1, 1): 1, (1, 1, 1, 0, 0, 1): 1, (1, 0, 0, 0, 1, 1): 1, (0, 0, 0, 1, 0, 1): 1, (1, 1, 0, 0, 0, 1): 1, (0, 1, 0, 1, 1, 1): 1, (1, 0, 0, 0, 1, 0): 1, (1, 0, 0, 1, 0, 1): 1, (0, 0, 0, 1, 1, 1): 1, (1, 1, 0, 1, 0, 1): 1, (0, 0, 0, 0, 1, 0): 1, (0, 1, 1, 0, 0, 1): 1, (1, 0, 1, 1, 0, 1): 1})\n",
      "Validation set distribution: Counter({(0, 0, 1, 1, 0, 1): 2, (0, 0, 0, 0, 0, 0): 1, (0, 0, 0, 1, 0, 0): 1, (0, 0, 1, 0, 0, 1): 1, (0, 1, 0, 1, 0, 0): 1, (0, 0, 1, 1, 1, 1): 1, (1, 0, 0, 1, 0, 0): 1, (0, 0, 1, 0, 1, 1): 1, (0, 1, 1, 1, 0, 1): 1})\n",
      "Test set distribution: Counter({(0, 0, 0, 0, 0, 0): 2, (0, 0, 1, 1, 0, 1): 2, (0, 0, 0, 0, 0, 1): 1, (0, 1, 0, 1, 0, 0): 1, (1, 0, 1, 0, 0, 1): 1, (1, 0, 0, 1, 1, 1): 1, (1, 0, 1, 1, 1, 1): 1, (0, 1, 1, 1, 0, 1): 1, (0, 0, 0, 1, 0, 0): 1, (1, 0, 0, 0, 0, 0): 1, (0, 0, 0, 1, 1, 0): 1, (0, 0, 1, 1, 1, 1): 1, (0, 0, 1, 0, 1, 1): 1, (1, 0, 0, 1, 0, 0): 1})\n"
     ]
    }
   ],
   "source": [
    "flags_train_tuples = [tuple(flag) for flag in y_train]\n",
    "flags_val_tuples = [tuple(flag) for flag in y_val]\n",
    "flags_test_tuples = [tuple(flag) for flag in y_test]\n",
    "\n",
    "print(\"Train set distribution:\", Counter(flags_train_tuples))\n",
    "\n",
    "# Check class distribution in validation set\n",
    "print(\"Validation set distribution:\", Counter(flags_val_tuples))\n",
    "\n",
    "# Check class distribution in test set\n",
    "print( \"Test set distribution:\",Counter(flags_test_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged set distribution:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combination</th>\n",
       "      <th>Count_training</th>\n",
       "      <th>Count_val</th>\n",
       "      <th>Count_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1, 1, 0, 1)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 0, 1, 1, 0)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 1, 1, 1, 1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1, 0, 0, 0, 0)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 0, 1, 0, 0)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, 0, 1, 1, 1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0, 1, 0, 0, 1)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1, 0, 1, 0, 0)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0, 0, 0, 0, 0)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(1, 0, 1, 0, 1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(0, 0, 1, 0, 1)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(0, 1, 0, 1, 1)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(0, 0, 1, 1, 1)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1, 1, 0, 0, 1)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(0, 1, 1, 1, 1)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(0, 0, 0, 0, 1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(1, 1, 0, 1, 1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(1, 0, 0, 1, 1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(1, 0, 0, 0, 1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(1, 0, 0, 1, 0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(0, 0, 0, 1, 0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(1, 1, 1, 0, 1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Combination  Count_training  Count_val  Count_test\n",
       "0   (0, 1, 1, 0, 1)             8.0        1.0         2.0\n",
       "1   (0, 0, 1, 1, 0)             2.0        0.0         1.0\n",
       "2   (1, 1, 1, 1, 1)             1.0        0.0         1.0\n",
       "3   (1, 0, 0, 0, 0)             2.0        1.0         0.0\n",
       "4   (0, 0, 1, 0, 0)             6.0        1.0         2.0\n",
       "5   (1, 0, 1, 1, 1)             1.0        0.0         1.0\n",
       "6   (0, 1, 0, 0, 1)             2.0        0.0         1.0\n",
       "7   (1, 0, 1, 0, 0)             2.0        0.0         1.0\n",
       "8   (0, 0, 0, 0, 0)             4.0        1.0         1.0\n",
       "9   (1, 0, 1, 0, 1)             1.0        0.0         1.0\n",
       "10  (0, 0, 1, 0, 1)             2.0        0.0         0.0\n",
       "11  (0, 1, 0, 1, 1)             4.0        1.0         0.0\n",
       "12  (0, 0, 1, 1, 1)             2.0        0.0         0.0\n",
       "13  (1, 1, 0, 0, 1)             2.0        1.0         0.0\n",
       "14  (0, 1, 1, 1, 1)             3.0        0.0         1.0\n",
       "15  (0, 0, 0, 0, 1)             1.0        0.0         1.0\n",
       "16  (1, 1, 0, 1, 1)             0.0        1.0         0.0\n",
       "17  (1, 0, 0, 1, 1)             0.0        1.0         0.0\n",
       "18  (1, 0, 0, 0, 1)             0.0        1.0         0.0\n",
       "19  (1, 0, 0, 1, 0)             0.0        1.0         0.0\n",
       "20  (0, 0, 0, 1, 0)             0.0        1.0         0.0\n",
       "21  (1, 1, 1, 0, 1)             0.0        1.0         0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\" Combinations distribution within the different sets\"\n",
    "# Count occurrences for each dataset\n",
    "train_counts = Counter(map(tuple, y_train))\n",
    "val_counts = Counter(map(tuple, y_val))\n",
    "test_counts = Counter(map(tuple, y_test))\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame(list(train_counts.items()), columns=['Combination', 'Count_training'])\n",
    "val_df = pd.DataFrame(list(val_counts.items()), columns=['Combination', 'Count_val'])\n",
    "test_df = pd.DataFrame(list(test_counts.items()), columns=['Combination', 'Count_test'])\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(train_df, val_df, on='Combination', how='outer').merge(test_df, on='Combination', how='outer')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(\"Merged set distribution:\")\n",
    "display(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 0, 1, 0, 1, 1): 5, (0, 0, 1, 1, 0, 1): 5, (0, 1, 0, 1, 0, 0): 4, (0, 0, 0, 1, 0, 0): 4, (0, 0, 0, 0, 0, 0): 3, (1, 0, 0, 1, 0, 0): 3, (0, 1, 1, 1, 0, 1): 3, (0, 0, 1, 1, 1, 1): 3, (1, 0, 1, 0, 0, 1): 2, (1, 0, 0, 0, 0, 0): 2, (0, 0, 1, 0, 0, 1): 2, (0, 0, 0, 0, 0, 1): 2, (0, 0, 0, 1, 1, 0): 2, (1, 0, 1, 1, 1, 1): 2, (0, 1, 0, 1, 1, 0): 1, (1, 1, 1, 0, 0, 1): 1, (1, 0, 0, 0, 1, 1): 1, (0, 0, 0, 1, 0, 1): 1, (1, 1, 0, 0, 0, 1): 1, (0, 1, 0, 1, 1, 1): 1, (1, 0, 0, 0, 1, 0): 1, (1, 0, 0, 1, 0, 1): 1, (0, 0, 0, 1, 1, 1): 1, (1, 1, 0, 1, 0, 1): 1, (0, 0, 0, 0, 1, 0): 1, (1, 0, 1, 1, 0, 1): 1})\n",
      "Counter({(0, 0, 1, 1, 0, 1): 3, (0, 0, 0, 0, 0, 0): 3, (1, 0, 0, 1, 1, 1): 2, (0, 1, 0, 1, 0, 0): 1, (0, 1, 0, 1, 0, 1): 1, (1, 0, 1, 0, 1, 1): 1, (0, 0, 1, 1, 1, 1): 1, (1, 0, 0, 0, 0, 0): 1, (0, 1, 1, 0, 0, 1): 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "# def proportions(flags_test):\n",
    "#     fp1 = np.sum(flags_test[:,0])\n",
    "#     fp2 = np.sum(flags_test[:,1])\n",
    "#     fp3 = np.sum(flags_test[:,2])\n",
    "#     ff1 = np.sum(flags_test[:,3])\n",
    "#     ff2 = np.sum(flags_test[:,4])\n",
    "#     ff3 = np.sum(flags_test[:,5])\n",
    "\n",
    "#     return (fp1,fp2,fp3,ff1,ff2,ff3)\n",
    "# fp1,fp2,fp3,ff1,ff2,ff3 = proportions(flags_test)\n",
    "# fp1t,fp2t,fp3t,ff1t,ff2t,ff3t = proportions(flags_train)\n",
    "# # fp1v,fp2v,fp3v,ff1v,ff2v,ff3v = proportions(flags_val)\n",
    "# # trainp =fp1+fp1t+fp1v + fp2 +fp2v+fp2t + fp3+fp3v+fp3t\n",
    "# # print( \" proportions flags in test\",proportions(flags_test))\n",
    "# # print( \" proportions flags in train\",proportions(flags_train))\n",
    "# # print( \" proportions flags in val\",proportions(flags_val))\n",
    "# (fp1+fp2+fp3)/ len(flags_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import itertools\n",
    "\n",
    "# all_combinations = list(itertools.product([0, 1], repeat=6))\n",
    "# all_combinations = np.array(all_combinations)\n",
    "# # Initialize counters for each set\n",
    "# train_counter = Counter()\n",
    "# val_counter = Counter()\n",
    "# test_counter = Counter()\n",
    "# # Count occurrences in each set\n",
    "# for combination in all_combinations:\n",
    "#     train_counter[tuple(combination)] = np.sum(np.all(flags_train == combination, axis=1))\n",
    "#     val_counter[tuple(combination)] = np.sum(np.all(flags_val == combination, axis=1))\n",
    "#     test_counter[tuple(combination)] = np.sum(np.all(flags_val == combination, axis=1))\n",
    "\n",
    "# # Calculate proportions\n",
    "# total_train_samples = len(flags_train)\n",
    "# total_val_samples = len(flags_val)\n",
    "# total_test_samples = len(flags_test)\n",
    "\n",
    "# train_proportions = {k: v / total_train_samples for k, v in train_counter.items()}\n",
    "# val_proportions = {k: v / total_val_samples for k, v in val_counter.items()}\n",
    "# test_proportions = {k: v / total_test_samples for k, v in test_counter.items()}\n",
    "\n",
    "# print(\"Train Proportions:\", train_proportions)\n",
    "# print(\"Validation Proportions:\", val_proportions)\n",
    "# print(\"Test Proportions:\", test_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(train_proportions.keys())\n",
    "train_values = list(train_proportions.values())\n",
    "val_values = list(val_proportions.values())\n",
    "test_values = list(test_proportions.values())\n",
    "\n",
    "# Set up the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot a pie chart for the train set\n",
    "ax.pie(train_values, labels=labels, autopct='%1.1f%%', startangle=90, colors=['blue', 'green', 'red', 'purple', 'orange', 'pink'])\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# Add a title\n",
    "plt.title('Train Set Class Combinations Proportions')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_val_diff = np.abs(np.array(train_values) - np.array(val_values)) / np.array(train_values) * 100\n",
    "\n",
    "# Calcola la differenza percentuale tra le proporzioni di train e test\n",
    "train_test_diff = np.abs(np.array(train_values) - np.array(test_values)) / np.array(train_values) * 100\n",
    "\n",
    "# Stampa le differenze percentuali\n",
    "print(\"Differenza percentuale tra train e validation:\", train_val_diff)\n",
    "print(\"Differenza percentuale tra train e test:\", train_test_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 14, 2, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patient_names),len(patient_names_train), len(patient_names_val), len(patient_names_test)\n",
    "\n",
    "patient_names = ['MAD_21_0', 'MAD_31_0', 'MAD_62_0', 'MAD_62_0', 'MAD_91_0', 'MAD_65_0', 'MAD_53_0', 'MAD_39_0', 'MAD_146_0', 'MAD_3_0', 'MAD_112_0', 'MAD_106_0', 'MAD_108_0', 'MAD_84_0', 'MAD_21_0']\n",
    "\n",
    "target_values = ['MAD_21_0', 'MAD_62_0']\n",
    "\n",
    "# Find all indices for each target value and flatten the list\n",
    "indices = [index for value in target_values for index, name in enumerate(patient_names) if name == value]\n",
    "indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('deepvalve')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e425331017aa2792a787d47e87be0499f98464e06ac26bef7ff6ffb74fe506b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
